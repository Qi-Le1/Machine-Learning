{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn as sk\n",
    "import numpy as np\n",
    "import statistics\n",
    "from sklearn import datasets\n",
    "X, t = sk.datasets.load_boston(return_X_y=True)\n",
    "Y, u = sk.datasets.load_digits(n_class=10, return_X_y=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 1]\n",
      " [1 1]]\n",
      "[3 7]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "[0.]\n",
      "11\n"
     ]
    }
   ],
   "source": [
    "a = [[2,1],[3,4]]\n",
    "\n",
    "print(np.sign(a))\n",
    "print(np.sum(a,axis=1))\n",
    "\n",
    "mean = list(range(10))\n",
    "print(mean)\n",
    "\n",
    "aa = np.zeros(1)\n",
    "print(aa)\n",
    "if aa == 0:\n",
    "    print(\"11\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\lucky\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\ipykernel_launcher.py:90: RuntimeWarning: overflow encountered in exp\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([2.59044821]), 0, 0, 0, 0, 0]\n",
      "[array([2.59044821]), array([6.49752025]), 0, 0, 0, 0]\n",
      "[array([2.59044821]), array([6.49752025]), array([7.18141287]), 0, 0, 0]\n",
      "[array([2.59044821]), array([6.49752025]), array([7.18141287]), array([11.17120808]), 0, 0]\n",
      "[array([2.59044821]), array([6.49752025]), array([7.18141287]), array([11.17120808]), array([11.51318015]), 0]\n"
     ]
    }
   ],
   "source": [
    "import sklearn as sk\n",
    "import numpy as np\n",
    "import statistics\n",
    "from sklearn import datasets\n",
    "X, t = sk.datasets.load_boston(return_X_y=True)\n",
    "Y, u = sk.datasets.load_digits(n_class=10, return_X_y=True)\n",
    "\n",
    "\n",
    "def trainTestSplit_Boston(X, t, cut, test_size):\n",
    "    X_num = len(X)\n",
    "    train_index = []\n",
    "    for i in range(X_num):\n",
    "        train_index.append(i)\n",
    "\n",
    "    test_index = []\n",
    "    test_num = int(X_num * test_size)\n",
    "    size = X_num - test_num\n",
    "\n",
    "    X_train = np.zeros((size, 13))\n",
    "    t_train = np.zeros((size, 1))\n",
    "\n",
    "    X_test = np.zeros((test_num, 13))\n",
    "    t_test = np.zeros((test_num, 1))\n",
    "\n",
    "    for i in range(test_num):\n",
    "        randomIndex = int(np.random.uniform(0, len(train_index)))\n",
    "        test_index.append(train_index[randomIndex])\n",
    "        del train_index[randomIndex]\n",
    "\n",
    "    j = 0\n",
    "    for i in train_index:\n",
    "        X_train[j] = X[i]\n",
    "        if (t[i] >= cut):\n",
    "            t_train[j] = 1\n",
    "        else:\n",
    "            t_train[j] = 0\n",
    "        j += 1\n",
    "\n",
    "    j = 0\n",
    "    for i in test_index:\n",
    "        X_test[j] = X[i]\n",
    "        if (t[i] >= cut):\n",
    "            t_test[j] = 1\n",
    "        else:\n",
    "            t_test[j] = 0\n",
    "        j += 1\n",
    "\n",
    "    return X_train, t_train, X_test, t_test\n",
    "\n",
    "\n",
    "def trainTestSplit_digit(X, t, test_size):\n",
    "    shape = (64, 1)\n",
    "\n",
    "    X_num = len(X)\n",
    "    train_index = []\n",
    "    for i in range(X_num):\n",
    "        train_index.append(i)\n",
    "\n",
    "    test_index = []\n",
    "    test_num = int(X_num * test_size)\n",
    "    size = X_num - test_num\n",
    "\n",
    "    X_train = np.zeros((size, 64))\n",
    "    t_train = np.zeros((size, 1))\n",
    "\n",
    "    X_test = np.zeros((test_num, 64))\n",
    "    t_test = np.zeros((test_num, 1))\n",
    "\n",
    "    for i in range(test_num):\n",
    "        randomIndex = int(np.random.uniform(0, len(train_index)))\n",
    "        test_index.append(train_index[randomIndex])\n",
    "        del train_index[randomIndex]\n",
    "\n",
    "    j = 0\n",
    "    for i in train_index:\n",
    "        X_train[j] = X[i]\n",
    "        t_train[j] = t[i]\n",
    "        j += 1\n",
    "\n",
    "    j = 0\n",
    "    for i in test_index:\n",
    "        X_test[j] = X[i]\n",
    "        t_test[j] = t[i]\n",
    "        j += 1\n",
    "\n",
    "    return X_train, t_train, X_test, t_test\n",
    "\n",
    "def trainset_percentage(X, t, sep_ratio):\n",
    "    # Separate data into training set and testing set.\n",
    "    X_train_size = int(len(X) * (sep_ratio / 100))\n",
    "\n",
    "    return X[:X_train_size], t[:X_train_size], X_train_size\n",
    "\n",
    "\n",
    "def sigmoid(z):  # Sigmoid Function\n",
    "    for i in range(len(z)):                             #prevent from overflow\n",
    "        if z[i] <=-400:\n",
    "            z[i] = -400\n",
    "\n",
    "    return np.clip(1 / (1.0 + np.exp(-z)), 1e-15, 1 - (1e-15))\n",
    "\n",
    "def f(X, w, b):\n",
    "\n",
    "    return sigmoid(np.matmul(X, w) + b)\n",
    "\n",
    "def gradient(X, Y_label, w, b):\n",
    "    # This function computes the gradient of cross entropy loss with respect to weight w and bias b.\n",
    "    y_pred = f(X, w, b)\n",
    "\n",
    "    pred_error = Y_label - y_pred\n",
    "    # print(pred_error)\n",
    "    w_grad = -np.dot(X.T, pred_error)\n",
    "    b_grad = -np.sum(pred_error)\n",
    "    return w_grad, b_grad\n",
    "\n",
    "def cross_entropy_loss(y_pred, Y_label):\n",
    "    # print('a',y_pred)\n",
    "    # print('b',np.log(y_pred))\n",
    "    lambda1 = 0.08\n",
    "    cross_entropy = 0\n",
    "    for i in range(len(y_pred)):\n",
    "        if Y_label[i] == 1:\n",
    "            cross_entropy -= (Y_label[i] * np.log(y_pred[i]))\n",
    "        else:\n",
    "            cross_entropy -= ((1 - Y_label[i]) * np.log(1 - y_pred[i]))\n",
    "\n",
    "    #     cross_entropy = -np.dot(Y_label, np.log(y_pred))\n",
    "    #     b = - np.dot((1 - Y_label), np.log(1 - y_pred))\n",
    "    return cross_entropy\n",
    "\n",
    "def multi_gradient(X, Y_label, w, b):\n",
    "    # This function computes the gradient of cross entropy loss with respect to weight w and bias b.\n",
    "    w_grad = np.zeros((64, 10))\n",
    "    b_grad = np.zeros((10, 1))\n",
    "    row = len(Y_label)\n",
    "\n",
    "    different_number_count = np.zeros((row, 10))\n",
    "\n",
    "    #print('different_number_count11111', different_number_count, different_number_count.shape)\n",
    "    for i in range(row):\n",
    "        if Y_label[i] == 0:\n",
    "            different_number_count[i][0] = 1\n",
    "        elif Y_label[i] == 1:\n",
    "            different_number_count[i][1] = 1\n",
    "        elif Y_label[i] == 2:\n",
    "            different_number_count[i][2] = 1\n",
    "        elif Y_label[i] == 3:\n",
    "            different_number_count[i][3] = 1\n",
    "        elif Y_label[i] == 4:\n",
    "            different_number_count[i][4] = 1\n",
    "        elif Y_label[i] == 5:\n",
    "            different_number_count[i][5] = 1\n",
    "        elif Y_label[i] == 6:\n",
    "            different_number_count[i][6] = 1\n",
    "        elif Y_label[i] == 7:\n",
    "            different_number_count[i][7] = 1\n",
    "        elif Y_label[i] == 8:\n",
    "            different_number_count[i][8] = 1\n",
    "        elif Y_label[i] == 9:\n",
    "            different_number_count[i][9] = 1\n",
    "\n",
    "    for row in range(row):\n",
    "\n",
    "        for column in range(10):\n",
    "            y_pred = softmax(X, w, b, column,row)   #number\n",
    "            prediction_error = different_number_count[row][column] - y_pred  #number\n",
    "            #print(prediction_error)\n",
    "            w_grad[:,column] -= X[row,:]*prediction_error\n",
    "            b_grad[column] -= prediction_error\n",
    "        # grad = - np.dot(X.T, prediction_error)\n",
    "        #\n",
    "        # for pred in range(64):\n",
    "        #     w_grad[pred,i]=grad[pred,0]\n",
    "        #b_grad[i] = - np.sum(prediction_error)\n",
    "        #print(w_grad.shape)\n",
    "    return w_grad, b_grad\n",
    "\n",
    "def softmax(data, w, b, column,row):\n",
    "    datapoiont_number = len(data)\n",
    "\n",
    "    X = data[row, :]\n",
    "\n",
    "    denominator = 0\n",
    "    nominator = 0\n",
    "\n",
    "    for i in range(10):\n",
    "        exp_content = np.dot(X, w[:, i]) + b[i]\n",
    "        for j in range(len(exp_content)):\n",
    "            if exp_content[j] >= 600:\n",
    "                exp_content[j] = 600\n",
    "        denominator += np.exp(exp_content)\n",
    "\n",
    "    # nominator\n",
    "    exp_content = np.matmul(X, w[:, column]) + b[column]\n",
    "    for j in range(len(exp_content)):\n",
    "        if exp_content[j] >= 600:\n",
    "            exp_content[j] = 600\n",
    "    nominator= np.exp(exp_content)\n",
    "\n",
    "    return nominator / denominator\n",
    "\n",
    "def loss_softmax(data, w, b, current_num,current_data):\n",
    "\n",
    "    denominator = np.zeros((1,1))\n",
    "    nominator = np.zeros((1, 1))\n",
    "\n",
    "    X = data[current_data,:]\n",
    "\n",
    "\n",
    "    for i in range(10):\n",
    "\n",
    "        exp_content = np.dot(X, w[:, i]) + b[i]\n",
    "        #.reshape(64, 1)\n",
    "        for j in range(len(exp_content)):\n",
    "            if exp_content[j] >= 600:\n",
    "                exp_content[j] = 600\n",
    "        # for j in range(datapoiont_number):\n",
    "        denominator[:,0] += np.exp(exp_content)\n",
    "\n",
    "    exp_content = np.dot(X, w[:, int(current_num)]) + b[int(current_num)]\n",
    "    for j in range(len(exp_content)):\n",
    "        if exp_content[j] >= 600:\n",
    "            exp_content[j] = 600\n",
    "\n",
    "    nominator[:,0] = np.exp(exp_content)\n",
    "\n",
    "    return nominator / denominator\n",
    "\n",
    "def cross_entropy_loss_multiclass(X, Y_label,w,b):\n",
    "\n",
    "    cross_entropy_loss_multi = 0\n",
    "    data_num = len(X)\n",
    "\n",
    "    for i in range(data_num):\n",
    "        current_num = Y_label[i]\n",
    "        #[i].reshape(1,64).shape\n",
    "        y_pred = loss_softmax(X,w,b,current_num,i)\n",
    "        if(y_pred == 0):\n",
    "            y_pred = 0.000001\n",
    "        cross_entropy_loss_multi -= np.log(y_pred)\n",
    "        #print('y_pred',y_pred)\n",
    "        #print('np.log(y_pred)', np.log(y_pred))\n",
    "\n",
    "    return cross_entropy_loss_multi\n",
    "\n",
    "def accuracy(Y_pred, Y_label):\n",
    "    # prediction accuracy\n",
    "    acc = 1 - np.mean(np.abs(Y_pred - Y_label))\n",
    "    return acc\n",
    "\n",
    "\n",
    "median = np.median(t)  # Fine the median number\n",
    "seventy_five_percentile = np.percentile(t, 75)  # Find the 75th percentile number\n",
    "iteration = 100\n",
    "learning_rate = 0.2\n",
    "\n",
    "evaluation_number = 1\n",
    "vector = [50]\n",
    "train_loss = [0, 0, 0, 0, 0, 0]\n",
    "test_loss = [0, 0, 0, 0, 0, 0]\n",
    "train_acc = [0,0,0,0,0]\n",
    "digit_train_loss = [0,0,0,0,0]\n",
    "for i in range(1, evaluation_number + 1):\n",
    "    # Randomize the 80-20 split\n",
    "    X_train_50, t_train_50, X_test_50, t_test_50 = trainTestSplit_Boston(X, t, median, 0.2)\n",
    "    X_train_75, t_train_75, X_test_75, t_test_75 = trainTestSplit_Boston(X, t, seventy_five_percentile, 0.2)\n",
    "    Y_train_digit, u_train_digit, Y_test_digit, u_test_digit = trainTestSplit_digit(Y, u, 0.2)\n",
    "\n",
    "    vector_order = 0\n",
    "    # print(X_train_50.shape)\n",
    "    for j in vector:\n",
    "        X_train_50_j, t_train_50_j, Boston_train_size_50 = trainset_percentage(X_train_50, t_train_50, j)\n",
    "        X_train_75_j, t_train_75_j, Boston_train_size_75 = trainset_percentage(X_train_75, t_train_75, j)\n",
    "        Y_train_digit_j, u_train_digit_j, digit_train_size = trainset_percentage(Y_train_digit, u_train_digit, j)\n",
    "\n",
    "        w_Boston_50 = np.zeros((13, 1))\n",
    "        b_Boston_50 = np.zeros((1,))\n",
    "        # b_Boston_50 = np.zeros((Boston_train_size_50,1))\n",
    "\n",
    "        w_Boston_75 = np.zeros((13, 1))\n",
    "\n",
    "        w_digit = np.zeros((64, 10))\n",
    "        b_digit = np.zeros((10, 1))\n",
    "\n",
    "        time = 1\n",
    "\n",
    "        for o in range(iteration):\n",
    "            w_grad_50, b_grad_50 = gradient(X_train_50_j, t_train_50_j, w_Boston_50, b_Boston_50)\n",
    "\n",
    "            w_Boston_50 = w_Boston_50 - learning_rate / np.sqrt(time) * w_grad_50\n",
    "            b_Boston_50 = b_Boston_50 - learning_rate / np.sqrt(time) * b_grad_50\n",
    "\n",
    "            #             w_grad_75, b_grad_75 = gradient(X_train_75_j, t_train_75_j, w_Boston_75, b_Boston_75)\n",
    "\n",
    "            #             w_Boston_75 = w_Boston_75 - learning_rate/np.sqrt(time) * w_grad_75\n",
    "            #             b_Boston_75 = b_Boston_75 - learning_rate/np.sqrt(time) * b_grad_75\n",
    "\n",
    "            w_grad_digit, b_grad_digit = multi_gradient(Y_train_digit_j, u_train_digit_j, w_digit, b_digit)\n",
    "\n",
    "            w_digit = w_digit - learning_rate / np.sqrt(time) * w_grad_digit\n",
    "            b_digit = b_digit - learning_rate / np.sqrt(time) * b_grad_digit\n",
    "            #print('sssss', w_digit)\n",
    "            time = time + 1\n",
    "        #         #print(X_train_50_j.shape)\n",
    "\n",
    "        y_train_pred = f(X_train_50_j, w_Boston_50, b_Boston_50)\n",
    "        Y_train_pred = np.round(y_train_pred)\n",
    "        train_acc.append(accuracy(Y_train_pred, t_train_50_j))\n",
    "        train_loss[vector_order] += (\n",
    "                    cross_entropy_loss(y_train_pred, t_train_50_j) / (len(X_train_50_j) * evaluation_number))\n",
    "        # print(\"a\", train_loss)\n",
    "\n",
    "        y_dev_pred = f(X_test_50, w_Boston_50, b_Boston_50)\n",
    "        Y_dev_pred = np.round(y_dev_pred)\n",
    "        # dev_acc.append(accuracy(Y_dev_pred, Y_dev))\n",
    "        test_loss[vector_order] += (\n",
    "                cross_entropy_loss(y_dev_pred, t_test_50) / (len(t_test_50) * evaluation_number))\n",
    "        print('train_loss',train_loss)\n",
    "        print('test_loss',test_loss)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        digit_train_loss[vector_order] += ( cross_entropy_loss_multiclass(Y_train_digit_j, u_train_digit_j, w_digit,b_digit)/ evaluation_number)\n",
    "        print('zheshi',digit_train_loss)\n",
    "#         y_dev_pred = _f(X_dev, w, b)\n",
    "#         Y_dev_pred = np.round(y_dev_pred)\n",
    "#         dev_acc.append(_accuracy(Y_dev_pred, Y_dev))\n",
    "        vector_order += 1\n",
    "#         step = step + 1\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     main()#train_loss[vector_order] += ( cross_entropy_loss(y_train_pred, t_train_50_j) / len(X_train_50_j))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
